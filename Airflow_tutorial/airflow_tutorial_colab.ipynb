{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fca1d850",
   "metadata": {},
   "source": [
    "# Apache Airflow Tutorial for Google Colab\n",
    "\n",
    "This notebook will help you learn Apache Airflow concepts and create your first DAG (Directed Acyclic Graph).\n",
    "\n",
    "**Note**: While Airflow webserver won't work in Colab, you can learn DAG structure and test task logic."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce7f68b2",
   "metadata": {},
   "source": [
    "## Step 1: Install Apache Airflow\n",
    "\n",
    "First, let's install Airflow in this Colab environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e7d2491",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Apache Airflow\n",
    "!pip install 'apache-airflow==3.0.3' \\\n",
    "    --constraint \"https://raw.githubusercontent.com/apache/airflow/constraints-3.0.3/constraints-3.12.txt\"\n",
    "\n",
    "# Install additional packages we'll need\n",
    "!pip install pandas matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3352b09",
   "metadata": {},
   "source": [
    "## Step 2: Set Up Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d9ecdf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "import pandas as pd\n",
    "\n",
    "# Set Airflow home directory\n",
    "os.environ['AIRFLOW_HOME'] = '/content/airflow'\n",
    "\n",
    "# Create directory structure\n",
    "!mkdir -p /content/airflow/dags\n",
    "!mkdir -p /content/airflow/logs\n",
    "!mkdir -p /content/airflow/plugins\n",
    "\n",
    "print(\"Airflow environment set up successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "357fb051",
   "metadata": {},
   "source": [
    "## Step 3: Initialize Airflow Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "777a9960",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Airflow database\n",
    "!airflow db init\n",
    "\n",
    "print(\"Airflow database initialized!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2085beab",
   "metadata": {},
   "source": [
    "## Step 4: Your First DAG\n",
    "\n",
    "Let's create a simple DAG that demonstrates common Airflow concepts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42305514",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple DAG file\n",
    "dag_code = '''\n",
    "from datetime import datetime, timedelta\n",
    "from airflow import DAG\n",
    "from airflow.operators.dummy import DummyOperator\n",
    "from airflow.operators.python import PythonOperator\n",
    "from airflow.operators.bash import BashOperator\n",
    "\n",
    "# Default arguments for the DAG\n",
    "default_args = {\n",
    "    'owner': 'student',\n",
    "    'depends_on_past': False,\n",
    "    'start_date': datetime(2025, 1, 1),\n",
    "    'email_on_failure': False,\n",
    "    'email_on_retry': False,\n",
    "    'retries': 1,\n",
    "    'retry_delay': timedelta(minutes=5),\n",
    "}\n",
    "\n",
    "# Define the DAG\n",
    "dag = DAG(\n",
    "    'tutorial_dag',\n",
    "    default_args=default_args,\n",
    "    description='A simple tutorial DAG',\n",
    "    schedule_interval=timedelta(days=1),\n",
    "    catchup=False,\n",
    "    tags=['tutorial'],\n",
    ")\n",
    "\n",
    "# Task 1: Start task\n",
    "start_task = DummyOperator(\n",
    "    task_id='start',\n",
    "    dag=dag,\n",
    ")\n",
    "\n",
    "# Task 2: Python function\n",
    "def hello_world():\n",
    "    print(\"Hello from Airflow!\")\n",
    "    return \"Task completed successfully\"\n",
    "\n",
    "python_task = PythonOperator(\n",
    "    task_id='hello_world_python',\n",
    "    python_callable=hello_world,\n",
    "    dag=dag,\n",
    ")\n",
    "\n",
    "# Task 3: Bash command\n",
    "bash_task = BashOperator(\n",
    "    task_id='hello_world_bash',\n",
    "    bash_command='echo \"Hello from Bash!\"',\n",
    "    dag=dag,\n",
    ")\n",
    "\n",
    "# Task 4: Data processing example\n",
    "def process_data():\n",
    "    import pandas as pd\n",
    "    \n",
    "    # Create sample data\n",
    "    data = {\n",
    "        'name': ['Alice', 'Bob', 'Charlie'],\n",
    "        'score': [95, 87, 92]\n",
    "    }\n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    # Process data\n",
    "    avg_score = df['score'].mean()\n",
    "    print(f\"Average score: {avg_score}\")\n",
    "    \n",
    "    return avg_score\n",
    "\n",
    "data_task = PythonOperator(\n",
    "    task_id='process_data',\n",
    "    python_callable=process_data,\n",
    "    dag=dag,\n",
    ")\n",
    "\n",
    "# Task 5: End task\n",
    "end_task = DummyOperator(\n",
    "    task_id='end',\n",
    "    dag=dag,\n",
    ")\n",
    "\n",
    "# Define task dependencies\n",
    "start_task >> [python_task, bash_task] >> data_task >> end_task\n",
    "'''\n",
    "\n",
    "# Write the DAG to a file\n",
    "with open('/content/airflow/dags/tutorial_dag.py', 'w') as f:\n",
    "    f.write(dag_code)\n",
    "\n",
    "print(\"DAG file created successfully!\")\n",
    "print(\"File location: /content/airflow/dags/tutorial_dag.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43ae495d",
   "metadata": {},
   "source": [
    "## Step 5: Test Your DAG\n",
    "\n",
    "Let's test if our DAG is valid and list it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4edd71d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all DAGs\n",
    "!airflow dags list\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"DAG Details:\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Show DAG details\n",
    "!airflow dags show tutorial_dag"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ceb21c2",
   "metadata": {},
   "source": [
    "## Step 6: Test Individual Tasks\n",
    "\n",
    "You can test individual tasks without running the full DAG:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7f31195",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the Python task\n",
    "!airflow tasks test tutorial_dag hello_world_python 2025-01-01\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "\n",
    "# Test the data processing task\n",
    "!airflow tasks test tutorial_dag process_data 2025-01-01"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ed7f5e5",
   "metadata": {},
   "source": [
    "## Step 7: Understanding DAG Structure\n",
    "\n",
    "Let's visualize and understand the DAG structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29780cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all tasks in the DAG\n",
    "!airflow tasks list tutorial_dag\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Task Dependencies:\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Show task dependencies\n",
    "!airflow tasks show tutorial_dag\n",
    "\n",
    "print(\"\\nDAG Flow:\")\n",
    "print(\"start → [hello_world_python, hello_world_bash] → process_data → end\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6411f320",
   "metadata": {},
   "source": [
    "## Step 8: Advanced DAG Example\n",
    "\n",
    "Let's create a more advanced DAG with conditional logic:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd1f7e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "advanced_dag_code = '''\n",
    "from datetime import datetime, timedelta\n",
    "from airflow import DAG\n",
    "from airflow.operators.python import PythonOperator, BranchPythonOperator\n",
    "from airflow.operators.dummy import DummyOperator\n",
    "\n",
    "default_args = {\n",
    "    'owner': 'student',\n",
    "    'start_date': datetime(2025, 1, 1),\n",
    "    'retries': 1,\n",
    "}\n",
    "\n",
    "dag = DAG(\n",
    "    'advanced_tutorial_dag',\n",
    "    default_args=default_args,\n",
    "    description='Advanced DAG with branching',\n",
    "    schedule_interval='@daily',\n",
    "    catchup=False,\n",
    ")\n",
    "\n",
    "def check_data_quality():\n",
    "    import random\n",
    "    quality_score = random.randint(1, 100)\n",
    "    print(f\"Data quality score: {quality_score}\")\n",
    "    \n",
    "    if quality_score > 80:\n",
    "        return 'process_high_quality_data'\n",
    "    else:\n",
    "        return 'clean_data'\n",
    "\n",
    "def process_high_quality_data():\n",
    "    print(\"Processing high quality data...\")\n",
    "    return \"High quality processing completed\"\n",
    "\n",
    "def clean_data():\n",
    "    print(\"Cleaning data before processing...\")\n",
    "    return \"Data cleaned successfully\"\n",
    "\n",
    "def final_processing():\n",
    "    print(\"Final data processing...\")\n",
    "    return \"All processing completed\"\n",
    "\n",
    "# Tasks\n",
    "start = DummyOperator(task_id='start', dag=dag)\n",
    "\n",
    "quality_check = BranchPythonOperator(\n",
    "    task_id='check_data_quality',\n",
    "    python_callable=check_data_quality,\n",
    "    dag=dag,\n",
    ")\n",
    "\n",
    "process_high_quality = PythonOperator(\n",
    "    task_id='process_high_quality_data',\n",
    "    python_callable=process_high_quality_data,\n",
    "    dag=dag,\n",
    ")\n",
    "\n",
    "clean_data_task = PythonOperator(\n",
    "    task_id='clean_data',\n",
    "    python_callable=clean_data,\n",
    "    dag=dag,\n",
    ")\n",
    "\n",
    "final_task = PythonOperator(\n",
    "    task_id='final_processing',\n",
    "    python_callable=final_processing,\n",
    "    dag=dag,\n",
    "    trigger_rule='none_failed_or_skipped',\n",
    ")\n",
    "\n",
    "# Dependencies\n",
    "start >> quality_check >> [process_high_quality, clean_data_task] >> final_task\n",
    "'''\n",
    "\n",
    "# Write the advanced DAG\n",
    "with open('/content/airflow/dags/advanced_tutorial_dag.py', 'w') as f:\n",
    "    f.write(advanced_dag_code)\n",
    "\n",
    "print(\"Advanced DAG created successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bd4787d",
   "metadata": {},
   "source": [
    "## Step 9: Key Airflow Concepts Summary\n",
    "\n",
    "**DAG (Directed Acyclic Graph)**: A collection of tasks with dependencies\n",
    "\n",
    "**Operators**: Define what actually gets done\n",
    "- `PythonOperator`: Runs Python functions\n",
    "- `BashOperator`: Runs bash commands\n",
    "- `DummyOperator`: Does nothing (useful for structure)\n",
    "- `BranchPythonOperator`: Conditional task execution\n",
    "\n",
    "**Task Dependencies**: Define execution order using `>>` or `<<`\n",
    "\n",
    "**Schedule Interval**: When the DAG runs\n",
    "- `@daily`, `@hourly`, `@weekly`\n",
    "- Cron expressions: `'0 2 * * *'` (daily at 2 AM)\n",
    "- Timedelta: `timedelta(hours=1)`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2c3bc5e",
   "metadata": {},
   "source": [
    "## Step 10: Next Steps\n",
    "\n",
    "To continue learning Airflow:\n",
    "\n",
    "1. **Try GitHub Codespaces** for a full Airflow experience with webserver\n",
    "2. **Explore more operators**: SqlOperator, EmailOperator, etc.\n",
    "3. **Learn about XComs** for task communication\n",
    "4. **Study Airflow Variables and Connections**\n",
    "5. **Practice with real data workflows**\n",
    "\n",
    "**For a complete Airflow environment, use GitHub Codespaces or a Linux environment!**"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
